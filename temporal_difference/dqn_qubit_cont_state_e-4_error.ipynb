{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import abc\n",
    "import tensorflow as tf\n",
    "import tf_agents as tf_ag\n",
    "import numpy as np\n",
    "\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.specs import array_spec, tensor_spec\n",
    "from tf_agents.environments import wrappers\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.trajectories import policy_step as ps\n",
    "from tf_agents.typing import types\n",
    "from tf_agents.policies import random_py_policy\n",
    "from tf_agents.policies import epsilon_greedy_policy\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.policies import py_policy\n",
    "from tf_agents.metrics import py_metrics\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.drivers import tf_driver\n",
    "from tf_agents.drivers import dynamic_episode_driver\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.policies import py_tf_eager_policy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import qutip\n",
    "from qutip import sigmap, expect\n",
    "\n",
    "import random\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from common.common import get_average_return\n",
    "from TD_algorithms import Dqn\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* System parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 3.5 # terminal time step\n",
    "max_steps = 30 # number of time steps\n",
    "nb_iterations = 2000\n",
    "fc_layer_params = ((100, 75))\n",
    "\n",
    "seed = 1989\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "checkpoint_dir = \"../checkpoints/DQN_continuous_state_e-4_error/\"\n",
    "policy_dir = \"../saved_policies/DQN_continuous_e-4_error/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create training and evaluation environments and define actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate environment\n",
    "from environments.qubit_env import QubitStateContEnv\n",
    "\n",
    "# define actions\n",
    "# TODO: actions can be continuous within a range of course \n",
    "# actions will be the change in the magnetic field\n",
    "max_Ω = 1\n",
    "\n",
    "nb_actions = 9\n",
    "zero_action = np.array(0, dtype = np.float32) # action 0\n",
    "plus_action = np.array(max_Ω, dtype = np.float32) # action 1\n",
    "minus_action = np.array(-max_Ω, dtype = np.float32) # action 2\n",
    "half_plus_action = np.array(max_Ω / 2, dtype = np.float32) # action 3\n",
    "half_minus_action = np.array(-max_Ω / 2, dtype = np.float32) # action 5\n",
    "quarter_plus_action = np.array(max_Ω / 4, dtype = np.float32) # action 6\n",
    "quarter_minus_action = np.array(-max_Ω / 4, dtype = np.float32) # action 7\n",
    "plus2_action = np.array(2 * max_Ω, dtype = np.float32) # action 8\n",
    "minus2_action = np.array(-2 * max_Ω, dtype = np.float32) # action 9\n",
    "\n",
    "actions = [quarter_plus_action, quarter_minus_action, half_plus_action, half_minus_action, zero_action, plus_action, minus_action, plus2_action, minus2_action]\n",
    "\n",
    "# shuffle them randomly to avoid bias\n",
    "random.shuffle(actions)\n",
    "\n",
    "# list of possible Rabi frequency values\n",
    "omegas = [-max_Ω, -3 * max_Ω / 4, -max_Ω / 4, -max_Ω / 2, 0, max_Ω / 2, max_Ω / 4, 3 * max_Ω / 4, max_Ω]\n",
    "\n",
    "environment = QubitStateContEnv(T, max_steps, max_Ω, actions, fidelity_threshold = 0.9999, verbose_fg = False, nb_actions = nb_actions)\n",
    "eval_environement = QubitStateContEnv(T, max_steps, max_Ω, actions, fidelity_threshold = 0.9999, verbose_fg = False, nb_actions = nb_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent, q_net, replay_buffer, avg_returns = Dqn(\n",
    "    environment, \n",
    "    eval_environement, \n",
    "    nb_iterations = nb_iterations, \n",
    "    learning_rate = 1e-3, \n",
    "    gamma = 0.99, \n",
    "    max_steps = max_steps, \n",
    "    epsilon_greedy = 0.1,\n",
    "    fc_layer_params = fc_layer_params,\n",
    "    # checkpoint_dir = checkpoint_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_policy(agent):\n",
    "    greedy_actions = []\n",
    "    optimal_omegas = []\n",
    "    states = []\n",
    "    fidelities = []\n",
    "    t, Ω = (0, 0)\n",
    "    optimal_policy = agent.policy\n",
    "\n",
    "    environment.reset()\n",
    "    tf_env = tf_py_environment.TFPyEnvironment(environment)\n",
    "    time_step = tf_env.reset()\n",
    "\n",
    "    states.append(environment._quantum_state)\n",
    "    fidelities.append(0.0)\n",
    "    \n",
    "    while t < max_steps: #and tf_env._episode_ended is False:\n",
    "        # get initial state (t, Ω)\n",
    "        omega_index = omegas.index(Ω)\n",
    "        \n",
    "        # select maximum value action\n",
    "        # use a neural network to get the estimate for the best action \n",
    "        # get the argmax of the action with the highest value\n",
    "        action_tensor = optimal_policy.action(time_step)\n",
    "        [action_index] = action_tensor[0].numpy()\n",
    "\n",
    "        # add action into array of actions\n",
    "        greedy_actions.append(action_index)\n",
    "\n",
    "        # get actual action from index\n",
    "        action = actions[action_index]\n",
    "\n",
    "        # apply action into the environment\n",
    "        time_step = tf_env.step(action_index)\n",
    "\n",
    "        states.append(environment._quantum_state)\n",
    "\n",
    "        # get the new state/observation after tha action\n",
    "        [[Ω, _, _, _]] = time_step.observation.numpy()\n",
    "\n",
    "        # apply field in tha array\n",
    "        optimal_omegas.append(Ω)\n",
    "\n",
    "        fidelities.append(environment._fidelity ** 2)\n",
    "\n",
    "        if environment._episode_ended:\n",
    "            break\n",
    "        \n",
    "    return (greedy_actions, optimal_omegas, states, fidelities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_actions, optimal_omegas, states, fidelities = extract_policy(agent)\n",
    "\n",
    "# append another omega so that the last step can be shown in the figure\n",
    "optimal_omegas.append(optimal_omegas[-1])\n",
    "\n",
    "population2 = expect(sigmap().dag() * sigmap(), states)\n",
    "population1 = 1 - population2\n",
    "\n",
    "time_span = np.arange(len(optimal_omegas))\n",
    "time_span = [t * T / max_steps for t in time_span]\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n",
    "fig.set_figheight(8)\n",
    "fig.set_figwidth(12)\n",
    "fig.suptitle(f'DQN {nb_iterations} iterations - Discrete action and hybrid state space - Fidelity 0.9999')\n",
    "\n",
    "ax1.step(time_span, optimal_omegas, where = 'post')\n",
    "ax1.set_ylabel(r\"$\\frac{\\Omega}{\\Omega_{max}}$\", rotation = 0, fontsize = 12)\n",
    "ax1.set_ylim((-1.1, 1.1))\n",
    "ax1.set_xlabel(f\"t\")\n",
    "ax1.set_title(\"(a)\", loc = \"right\", fontsize = 10)\n",
    "\n",
    "ax2.plot(time_span, fidelities)\n",
    "ax2.axhline(y = 0.9999, color = 'r', linestyle = '--', label = '0.9999')\n",
    "ax2.set_ylabel(\"Fidelity\", rotation = 90, fontsize = 12)\n",
    "ax2.set_xlabel(f\"t\")\n",
    "ax2.legend(loc = 'lower right')\n",
    "ax2.set_title(\"(b)\", loc = \"right\", fontsize = 10)\n",
    "\n",
    "ax4.plot(time_span, population1, label = r\"$P_1$\")\n",
    "ax4.plot(time_span, population2, label = r\"$P_2$\")\n",
    "ax4.set_ylabel(\"Populations\", rotation = 90, fontsize = 12)\n",
    "ax4.set_xlabel(f\"t\")\n",
    "ax4.set_title(\"(d)\", loc = \"right\", fontsize = 10)\n",
    "ax4.legend()\n",
    "\n",
    "time_span3 = range(len(avg_returns))\n",
    "ax3.plot(time_span3, avg_returns)\n",
    "ax3.set_ylabel(\"Avg. Rewards (10 episodes)\", rotation = 90, fontsize = 12)\n",
    "ax3.set_xlabel(f\"evaluations\")\n",
    "ax3.set_title(\"(c)\", loc = \"right\", fontsize = 10)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
