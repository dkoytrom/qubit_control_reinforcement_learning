{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import abc\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.specs import array_spec, tensor_spec\n",
    "from tf_agents.environments import wrappers\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.trajectories import policy_step as ps\n",
    "from tf_agents.typing import types\n",
    "from tf_agents.policies import random_py_policy\n",
    "from tf_agents.policies import epsilon_greedy_policy\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.policies import py_policy\n",
    "from tf_agents.metrics import py_metrics\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.drivers import tf_driver\n",
    "from tf_agents.drivers import dynamic_episode_driver\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import qutip\n",
    "from qutip import sigmap, expect\n",
    "\n",
    "import random\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from environments.qubit_env import QubitEnv, extract_policy\n",
    "from common.common import EpsilonGreedyPolicy\n",
    "from temporal_difference.TD_algorithms import qlearing_algorithm, sarsa_algorithm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* System parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 10\n",
    "N = 30 # number of time steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define actions and create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define actions\n",
    "# actions will be the change in the magnetic field\n",
    "\n",
    "max_Ω = 1\n",
    "zero_action = np.array(0, dtype = np.int32) # action 0\n",
    "plus_action = np.array(1 * max_Ω, dtype = np.int32) # action 1\n",
    "minus_action = np.array(-1 * max_Ω, dtype = np.int32) # action 2\n",
    "plus2_action = np.array(2 * max_Ω, dtype = np.int32) # action 3\n",
    "minus2_action = np.array(-2 * max_Ω, dtype = np.int32) # action 4\n",
    "\n",
    "nb_actions = 2\n",
    "actions = [plus2_action, minus2_action]\n",
    "omegas = [-max_Ω, 0, max_Ω]\n",
    "\n",
    "environment = QubitEnv(T, N, max_Ω, actions, fidelity_threshold = 0.99, verbose_fg = False, nb_actions = nb_actions, seed = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.7\n",
    "gamma = 0.99\n",
    "\n",
    "(Qtable_qlearning_500, qlearning_rewards_500) = qlearing_algorithm(\n",
    "    environment = environment,\n",
    "    omegas = omegas,\n",
    "    collect_policy = EpsilonGreedyPolicy,\n",
    "    nb_episodes = 500, \n",
    "    learning_rate = learning_rate,\n",
    "    discount = gamma, \n",
    "    max_steps = N, \n",
    "    nb_actions = nb_actions\n",
    ")\n",
    "\n",
    "(Qtable_qlearning_1000, qlearning_rewards_1000) = qlearing_algorithm(\n",
    "    environment = environment,\n",
    "    omegas = omegas,\n",
    "    collect_policy = EpsilonGreedyPolicy,\n",
    "    nb_episodes = 1000, \n",
    "    learning_rate = learning_rate,\n",
    "    discount = gamma, \n",
    "    max_steps = N, \n",
    "    nb_actions = nb_actions\n",
    ")\n",
    "\n",
    "(Qtable_qlearning_2000, qlearning_rewards_2000) = qlearing_algorithm(\n",
    "    environment = environment,\n",
    "    omegas = omegas,\n",
    "    collect_policy = EpsilonGreedyPolicy,\n",
    "    nb_episodes = 2000, \n",
    "    learning_rate = learning_rate,\n",
    "    discount = gamma, \n",
    "    max_steps = N, \n",
    "    nb_actions = nb_actions\n",
    ")\n",
    "\n",
    "(Qtable_qlearning_5000, qlearning_rewards_5000) = qlearing_algorithm(\n",
    "    environment = environment,\n",
    "    omegas = omegas,\n",
    "    collect_policy = EpsilonGreedyPolicy,\n",
    "    nb_episodes = 5000, \n",
    "    learning_rate = learning_rate,\n",
    "    discount = gamma, \n",
    "    max_steps = N, \n",
    "    nb_actions = nb_actions\n",
    ")\n",
    "\n",
    "(Qtable_qlearning_20000, qlearning_rewards_20000) = qlearing_algorithm(\n",
    "    environment = environment,\n",
    "    omegas = omegas,\n",
    "    collect_policy = EpsilonGreedyPolicy,\n",
    "    nb_episodes = 20000, \n",
    "    learning_rate = learning_rate,\n",
    "    discount = gamma, \n",
    "    max_steps = N, \n",
    "    nb_actions = nb_actions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(optimal_omegas, states, num_iterations, fidelities, avg_returns):\n",
    "    # append another omega so that the last step can be shown in the figure\n",
    "    optimal_omegas.append(optimal_omegas[-1])\n",
    "\n",
    "    population2 = expect(sigmap().dag() * sigmap(), states)\n",
    "    population1 = 1 - population2\n",
    "\n",
    "    time_span = np.arange(len(optimal_omegas))\n",
    "    time_span = [t * T / N for t in time_span]\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n",
    "    fig.set_figheight(8)\n",
    "    fig.set_figwidth(12)\n",
    "    fig.suptitle(f'Q-learning {num_iterations} episodes')\n",
    "\n",
    "    ax1.step(time_span, optimal_omegas, where = 'post')\n",
    "    ax1.set_ylabel(r\"$\\frac{\\Omega}{\\Omega_{max}}$\", rotation = 0, fontsize = 12)\n",
    "    ax1.set_ylim((-1.1, 1.1))\n",
    "    ax1.set_xlabel(f\"t\")\n",
    "    ax1.set_title(\"(a)\", loc = \"right\", fontsize = 10)\n",
    "\n",
    "    ax2.plot(time_span, fidelities)\n",
    "    ax2.axhline(y = 0.99, color = 'r', linestyle = '--', label = '0.99')\n",
    "    ax2.set_ylabel(\"Fidelity\", rotation = 90, fontsize = 12)\n",
    "    ax2.set_xlabel(f\"t\")\n",
    "    ax2.legend(loc = 'lower right')\n",
    "    ax2.set_title(\"(b)\", loc = \"right\", fontsize = 10)\n",
    "\n",
    "    ax4.plot(time_span, population1, label = r\"$P_1$\")\n",
    "    ax4.plot(time_span, population2, label = r\"$P_2$\")\n",
    "    ax4.set_ylabel(\"Populations\", rotation = 90, fontsize = 12)\n",
    "    ax4.set_xlabel(f\"t\")\n",
    "    ax4.set_title(\"(d)\", loc = \"right\", fontsize = 10)\n",
    "    ax4.legend()\n",
    "\n",
    "    time_span3 = range(len(avg_returns))\n",
    "    ax3.plot(time_span3, avg_returns)\n",
    "    ax3.set_ylabel(\"Avg. Return (10 episodes)\", rotation = 90, fontsize = 12)\n",
    "    ax3.set_xlabel(f\"evaluations\")\n",
    "    ax3.set_title(\"(c)\", loc = \"right\", fontsize = 10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_actions, optimal_omegas, states, fidelities = extract_policy(Qtable_qlearning_500, environment, actions, omegas, N)\n",
    "\n",
    "print_results(optimal_omegas, states, 500, fidelities, qlearning_rewards_500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_actions, optimal_omegas, states, fidelities = extract_policy(Qtable_qlearning_1000, environment, actions, omegas, N)\n",
    "\n",
    "print_results(optimal_omegas, states, 1000, fidelities, qlearning_rewards_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_actions, optimal_omegas, states, fidelities = extract_policy(Qtable_qlearning_2000, environment, actions, omegas, N)\n",
    "\n",
    "print_results(optimal_omegas, states, 2000, fidelities, qlearning_rewards_2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_actions, optimal_omegas, states, fidelities = extract_policy(Qtable_qlearning_5000, environment, actions, omegas, N)\n",
    "\n",
    "print_results(optimal_omegas, states, 5000, fidelities, qlearning_rewards_5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_actions, optimal_omegas, states, fidelities = extract_policy(Qtable_qlearning_20000, environment, actions, omegas, N)\n",
    "\n",
    "print_results(optimal_omegas, states, 10000, fidelities, qlearning_rewards_20000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
